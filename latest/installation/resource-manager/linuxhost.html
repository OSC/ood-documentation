

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>LinuxHost &mdash; Open OnDemand 3.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Cloudy Cluster" href="ccq.html" />
    <link rel="prev" title="Grid Engine" href="sge.html" />
 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-34776817-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-34776817-3');
</script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                3.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../requirements.html">Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../add-cluster-config.html">Cluster Configuration</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../cluster-config-schema.html">Cluster Config Schema v2</a></li>
<li class="toctree-l2"><a class="reference internal" href="torque.html">Torque</a></li>
<li class="toctree-l2"><a class="reference internal" href="slurm.html">Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lsf.html">LSF</a></li>
<li class="toctree-l2"><a class="reference internal" href="pbspro.html">PBS Professional</a></li>
<li class="toctree-l2"><a class="reference internal" href="sge.html">Grid Engine</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">LinuxHost</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#launch-any-software-on-target-host">Launch any software on target host</a></li>
<li class="toctree-l3"><a class="reference internal" href="#launch-specific-application-containers">Launch specific application containers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cluster-configuration">Cluster Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#enforce-resource-limits-on-the-target-host">Enforce resource limits on the target host</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#approach-1-systemd-user-slices">Approach #1: Systemd user slices</a></li>
<li class="toctree-l4"><a class="reference internal" href="#approach-2-libcgroup-cgroups">Approach #2: libcgroup cgroups</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#undetermined-state">Undetermined state</a></li>
<li class="toctree-l4"><a class="reference internal" href="#error-while-loading-shared-libraries">error while loading shared libraries</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-job-just-exists-with-no-errors">The job just exists with no errors.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#d-bus-errors">D-Bus errors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ccq.html">Cloudy Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="kubernetes.html">Kubernetes</a></li>
<li class="toctree-l2"><a class="reference internal" href="systemd.html">Systemd</a></li>
<li class="toctree-l2"><a class="reference internal" href="bin-override-example.html">A Working Example of a <code class="docutils literal notranslate"><span class="pre">bin_overrides</span></code> Script</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html">Test Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced-configs.html">Advanced Resource Manager Configurations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../how-tos/app-development/interactive/setup.html">Setup Interactive Apps</a></li>
</ul>
<p class="caption"><span class="caption-text">How-Tos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../customizations.html">Customizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how-tos/debug.html">Debugging and Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../enable-desktops.html">Enable Interactive Desktop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how-tos/app-development.html">App Development</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install-ihpc-apps.html">Install Other Interactive Apps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/tutorials-interactive-apps.html">Tutorials: Interactive Apps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/tutorials-passenger-apps.html">Tutorials: Passenger Apps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/tutorials-dashboard-apps.html">Developing The OOD Dashboard</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference.html">Configuration Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../security.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../version-policy.html">Versioning Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Open OnDemand</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../add-cluster-config.html">Cluster Configuration</a> &raquo;</li>
        
      <li>LinuxHost</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/OSC/ood-documentation/blob/latest/source/installation/resource-manager/linuxhost.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="linuxhost">
<span id="resource-manager-linuxhost"></span><h1>LinuxHost<a class="headerlink" href="#linuxhost" title="Permalink to this headline">Â¶</a></h1>
<p>The LinuxHost adapter facilitates launching jobs immediately without using a batch scheduler or resource manager. Use cases for this non-traditional job adapter include:</p>
<ul class="simple">
<li><p>Interactive work on login nodes, such as remote desktop or IDE</p></li>
<li><p>Providing an option to start an interactive session immediately, without any queue time imposed</p></li>
<li><p>Using OnDemand on systems that do not have a supported scheduler installed</p></li>
</ul>
<p>The adapter pieces together several tools to achieve behavior similar to what a scheduler offers:</p>
<dl class="simple">
<dt>ssh</dt><dd><p>connects from the web node to a configured host such as a login node.</p>
</dd>
<dt>tmux</dt><dd><p>Specially named <code class="docutils literal notranslate"><span class="pre">tmux</span></code> sessions offer the ability to rediscover running jobs</p>
</dd>
<dt>timeout</dt><dd><p>is used to set a âwalltimeâ after which the job is killed</p>
</dd>
<dt>pstree</dt><dd><p>is used to detect the jobâs parent <code class="docutils literal notranslate"><span class="pre">sinit</span></code> process so that it can be killed</p>
</dd>
<dt>singularity</dt><dd><p>containerization provides a PID namespace without requiring elevated privileges that ensures that all child processes are cleaned up when the job either times out or is killed</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are many recipes for managing processes on arbitrary Linux hosts, and we will be exploring others in future development of this adapter.</p>
</div>
<p>The adapter and target host can be configured to either:</p>
<ol class="arabic simple">
<li><p>Enable launching any software installed on the target host</p></li>
<li><p>Enable launching a specific application container</p></li>
</ol>
<p>The cluster configuration can be shared between both strategies and each interactive app plugin individually configured to use the appropriate strategy.</p>
<div class="section" id="launch-any-software-on-target-host">
<h2>Launch any software on target host<a class="headerlink" href="#launch-any-software-on-target-host" title="Permalink to this headline">Â¶</a></h2>
<ol class="arabic simple">
<li><p>Install on the target host a Singularity base OS image matching the OS of the target host. In the cluster configuration example below we use <code class="docutils literal notranslate"><span class="pre">/opt/ood/linuxhost_adapter/centos_7.6.sif</span></code> as the target host is running CentOS7.</p></li>
<li><p>See <a class="reference internal" href="#resource-manager-linuxhost-cluster-configuration"><span class="std std-ref">Cluster Configuration</span></a> below to add the cluster configuration file for the target host</p></li>
</ol>
<p>This base image is used to provide an unprivileged PID namespace when launching the app. To make the rest of the software installed on the host available to launch within the image, most/all of the host file system is bind-mounted into the running container. For this reason many existing interactive apps will just work when launched by the LinuxHost adapter.</p>
</div>
<div class="section" id="launch-specific-application-containers">
<h2>Launch specific application containers<a class="headerlink" href="#launch-specific-application-containers" title="Permalink to this headline">Â¶</a></h2>
<ol class="arabic simple">
<li><p>Install the specific application containers on the target host.</p></li>
<li><p>See <a class="reference internal" href="#resource-manager-linuxhost-cluster-configuration"><span class="std std-ref">Cluster Configuration</span></a> below to add the cluster configuration file for the target host. Specify a default application container for <code class="docutils literal notranslate"><span class="pre">singularity_image:</span></code> and <code class="docutils literal notranslate"><span class="pre">singularity_bindpath:</span></code> even if you will override it.</p></li>
<li><p>In each interactive app plugin, specify the image to launch and host directories to mount by setting the <code class="docutils literal notranslate"><span class="pre">native</span></code> attribute <code class="docutils literal notranslate"><span class="pre">singularity_container</span></code> and <code class="docutils literal notranslate"><span class="pre">singularity_bindpath</span></code>. In interactive app plugins these attributes may be set in the file <code class="docutils literal notranslate"><span class="pre">submit.yml</span></code>. For example:</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span><span class="w"></span>
<span class="nt">batch_connect</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vnc</span><span class="w"></span>
<span class="nt">script</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">native</span><span class="p">:</span><span class="w"></span>
<span class="w">     </span><span class="nt">singularity_bindpath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/fs,/home</span><span class="w"></span>
<span class="w">     </span><span class="nt">singularity_container</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/usr/local/modules/netbeans/netbeans_2019.sif</span><span class="w"></span>
</pre></div>
</div>
</div>
<div class="section" id="cluster-configuration">
<span id="resource-manager-linuxhost-cluster-configuration"></span><h2>Cluster Configuration<a class="headerlink" href="#cluster-configuration" title="Permalink to this headline">Â¶</a></h2>
<p>A YAML cluster configuration file for a Linux host looks like:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># /etc/ood/config/clusters.d/owens_login.yml</span><span class="w"></span>
<span class="nn">---</span><span class="w"></span>
<span class="nt">v2</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">title</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Owens&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;https://www.osc.edu/supercomputing/computing/owens&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">hidden</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">  </span><span class="nt">login</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;owens.osc.edu&quot;</span><span class="w"></span>
<span class="hll"><span class="w">  </span><span class="nt">job</span><span class="p">:</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">adapter</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;linux_host&quot;</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">submit_host</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;owens.osc.edu&quot;</span><span class="w">  </span><span class="c1"># This is the head for a login round robin</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">ssh_hosts</span><span class="p">:</span><span class="w"> </span><span class="c1"># These are the actual login nodes</span><span class="w"></span>
</span><span class="hll"><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">owens-login01.hpc.osc.edu</span><span class="w"></span>
</span><span class="hll"><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">owens-login02.hpc.osc.edu</span><span class="w"></span>
</span><span class="hll"><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">owens-login03.hpc.osc.edu</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">site_timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">7200</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">debug</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">singularity_bin</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/usr/bin/singularity</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">singularity_bindpath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc,/media,/mnt,/opt,/run,/srv,/usr,/var,/users</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">singularity_image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/opt/ood/linuxhost_adapter/centos_7.6.sif</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="c1"># Enabling strict host checking may cause the adapter to fail if the user&#39;s known_hosts does not have all the roundrobin hosts</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">strict_host_checking</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">tmux_bin</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/usr/bin/tmux</span><span class="w"></span>
</span></pre></div>
</div>
<p>with the following configuration options:</p>
<dl class="simple">
<dt>adapter</dt><dd><p>This is set to <code class="docutils literal notranslate"><span class="pre">linux_host</span></code>.</p>
</dd>
<dt>submit_host</dt><dd><p>The target execution host for jobs. May be the head for a login round robin. May also be âlocalhostâ.</p>
</dd>
<dt>ssh_hosts</dt><dd><p>All nodes the submit_host can DNS resolve to.</p>
</dd>
<dt>site_timeout</dt><dd><p>The number of seconds that a userâs job is allowed to run. Distinct from the length of time that a user selects.</p>
</dd>
<dt>debug</dt><dd><p>When set to <code class="docutils literal notranslate"><span class="pre">true</span></code> job scripts are written to <code class="docutils literal notranslate"><span class="pre">$HOME/tmp.UUID_tmux</span></code> and <code class="docutils literal notranslate"><span class="pre">$HOME/tmp.UUID_sing</span></code> for debugging purposes. When <code class="docutils literal notranslate"><span class="pre">false</span></code> those files are written to <code class="docutils literal notranslate"><span class="pre">/tmp</span></code> and deleted as soon as they have been read.</p>
</dd>
<dt>singularity_bin</dt><dd><p>The absolute path to the <code class="docutils literal notranslate"><span class="pre">singularity</span></code> executable on the execution host(s).</p>
</dd>
<dt>singularity_bindpath</dt><dd><p>The comma delimited list of paths to bind mount into the host; cannot simply be <code class="docutils literal notranslate"><span class="pre">/</span></code> because Singularity expects certain dot files in its containersâ root; defaults to: <code class="docutils literal notranslate"><span class="pre">/etc,/media,/mnt,/opt,/run,/srv,/usr,/var,/users</span></code>.</p>
</dd>
<dt>singularity_image</dt><dd><p>The absolute path to the Singularity image used when simply PID namespacing jobs; expected to be a base distribution image with no customizations.</p>
</dd>
<dt>strict_host_checking</dt><dd><p>When <code class="docutils literal notranslate"><span class="pre">false</span></code> the SSH options include <code class="docutils literal notranslate"><span class="pre">StrictHostKeyChecking=no</span></code> and <code class="docutils literal notranslate"><span class="pre">UserKnownHostsFile=/dev/null</span></code> this prevents jobs from failing to launch.</p>
</dd>
<dt>tmux_bin</dt><dd><p>The absolute path to the <code class="docutils literal notranslate"><span class="pre">tmux</span></code> executable on the execution host(s).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This adapter was designed with the primary goal of launching installed
software on the target host, not launching specific application containers.
As a result, even if your use of this adapter is reserved to launching
specific application containers, you currently must specify a value in the
cluster config for <code class="docutils literal notranslate"><span class="pre">singularity_bindpath</span></code> and <code class="docutils literal notranslate"><span class="pre">singularity_image</span></code>, even
if these will be specified in each interactive app plugin.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to communicate with the execution hosts the adapter uses SSH in
<code class="docutils literal notranslate"><span class="pre">BatchMode</span></code>. The adapter does not take a position on whether authentication
is performed by user owned passwordless keys, or host-based authentication;
however OSC has chosen to provide <a class="reference external" href="https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Host-based_Authentication">host based authentication</a>
to its users.</p>
</div>
</div>
<div class="section" id="enforce-resource-limits-on-the-target-host">
<h2>Enforce resource limits on the target host<a class="headerlink" href="#enforce-resource-limits-on-the-target-host" title="Permalink to this headline">Â¶</a></h2>
<p>By default the adapter does not limit the userâs CPU or memory utilization, only their âwalltimeâ. The following are two examples of ways to implement resource limits for the LinuxHost Adapter using cgroups.</p>
<div class="section" id="approach-1-systemd-user-slices">
<h3>Approach #1: Systemd user slices<a class="headerlink" href="#approach-1-systemd-user-slices" title="Permalink to this headline">Â¶</a></h3>
<p>With systemd it is possible to manage the resource limits of user logins through each userâs <a class="reference external" href="https://www.freedesktop.org/software/systemd/man/systemd.slice.html">slice</a>. The limits applied to a user slice are shared by all processes belonging to that user, this is not a per-job or per-node resource limit but a per-user limit. When setting the limits keep in mind the sum of all user limits is the max potential resource consumption on a single host.</p>
<p>First update the PAM stack to include the following line:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>session     required      pam_exec.so type=open_session /etc/security/limits.sh
</pre></div>
</div>
<p>This goes into a file used by the <code class="docutils literal notranslate"><span class="pre">sshd</span></code> PAM configs which on CentOS/RHEL default to <code class="docutils literal notranslate"><span class="pre">/etc/pam.d/password-auth-ac</span></code> and needs to be included in the proper position, after <code class="docutils literal notranslate"><span class="pre">pam_systemd.so</span></code>. Also set <code class="docutils literal notranslate"><span class="pre">pam_systemd.so</span></code> to <code class="docutils literal notranslate"><span class="pre">required</span></code>:</p>
<div class="highlight-none notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span>session     optional      pam_keyinit.so revoke
session     required      pam_limits.so
<span class="hll">session     required      pam_systemd.so
</span><span class="hll">session     required      pam_exec.so type=open_session /etc/security/limits.sh
</span>session     [success=1 default=ignore] pam_succeed_if.so service in crond quiet use_uid
session     required      pam_unix.so
session     optional      pam_sss.so
</pre></div>
</td></tr></table></div>
<p>The following example of <code class="docutils literal notranslate"><span class="pre">/etc/security/limits.sh</span></code> is used by OSC on interactive login nodes. Adjust <code class="docutils literal notranslate"><span class="pre">MemoryLimit</span></code> and <code class="docutils literal notranslate"><span class="pre">CPUQuota</span></code> to meet the needs of your site. See <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">systemd.resource-control</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">set</span> -e

<span class="nv">PAM_UID</span><span class="o">=</span><span class="k">$(</span>id -u <span class="s2">&quot;</span><span class="si">${</span><span class="nv">PAM_USER</span><span class="si">}</span><span class="s2">&quot;</span><span class="k">)</span>

<span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;</span><span class="si">${</span><span class="nv">PAM_SERVICE</span><span class="si">}</span><span class="s2">&quot;</span> <span class="o">=</span> <span class="s2">&quot;sshd&quot;</span> -a <span class="s2">&quot;</span><span class="si">${</span><span class="nv">PAM_UID</span><span class="si">}</span><span class="s2">&quot;</span> -ge <span class="m">1000</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
        /usr/bin/systemctl set-property <span class="s2">&quot;user-</span><span class="si">${</span><span class="nv">PAM_UID</span><span class="si">}</span><span class="s2">.slice&quot;</span> <span class="se">\</span>
                <span class="nv">MemoryAccounting</span><span class="o">=</span><span class="nb">true</span> <span class="nv">MemoryLimit</span><span class="o">=</span>64G <span class="se">\</span>
                <span class="nv">CPUAccounting</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
                <span class="nv">CPUQuota</span><span class="o">=</span><span class="m">700</span>%
<span class="k">fi</span>
</pre></div>
</div>
</div>
<div class="section" id="approach-2-libcgroup-cgroups">
<h3>Approach #2: libcgroup cgroups<a class="headerlink" href="#approach-2-libcgroup-cgroups" title="Permalink to this headline">Â¶</a></h3>
<p>The libcgroup cgroups rules and configurations are a per-group resource limit where the group is defined in the examples at <code class="docutils literal notranslate"><span class="pre">/etc/cgconfig.d/limits.conf</span></code>. The following examples limit resources of all tmux processes launched for the LinuxHost Adapter so they all share 700 CPU shares and 64GB of RAM. This requires setting <code class="docutils literal notranslate"><span class="pre">tmux_bin</span></code> to a wrapper script that in this example will be <code class="docutils literal notranslate"><span class="pre">/usr/local/bin/ondemand_tmux</span></code>.</p>
<p>Example of <code class="docutils literal notranslate"><span class="pre">/usr/local/bin/ondemand_tmux</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">exec</span> tmux <span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Setup the cgroup limits at <code class="docutils literal notranslate"><span class="pre">/etc/cgconfig.d/limits.conf</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>group linuxhostadapter {
        memory {
                memory.limit_in_bytes=&quot;64G&quot;;
                memory.memsw.limit_in_bytes=&quot;64G&quot;;
        }
        cpu {
                cpu.shares=&quot;700&quot;;
        }
}
</pre></div>
</div>
<p>Setup the cgroup rules at <code class="docutils literal notranslate"><span class="pre">/etc/cgrules.conf</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>*:/usr/local/bin/ondemand_tmux memory linuxhostadapter/
*:/usr/local/bin/ondemand_tmux cpu linuxhostadapter/
</pre></div>
</div>
<p>Start the necessary services:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>sudo systemctl start cgconfig
sudo systemctl start cgred
sudo systemctl <span class="nb">enable</span> cgconfig
sudo systemctl <span class="nb">enable</span> cgred
</pre></div>
</div>
</div>
</div>
<div class="section" id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="undetermined-state">
<h3>Undetermined state<a class="headerlink" href="#undetermined-state" title="Permalink to this headline">Â¶</a></h3>
<p>Your job can be in an âundetermined stateâ because you havenât listed all the <code class="docutils literal notranslate"><span class="pre">ssh_hosts</span></code>.
<code class="docutils literal notranslate"><span class="pre">ssh_hosts</span></code> should be <em>anything</em> the <code class="docutils literal notranslate"><span class="pre">submit_host</span></code> can DNS resolve to. You submit your
job the <code class="docutils literal notranslate"><span class="pre">submit_host</span></code>, but OnDemand is going to poll the <code class="docutils literal notranslate"><span class="pre">ssh_hosts</span></code> for your job and
in this case, your running a job on a node that OnDemand is not polling.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># /etc/ood/config/clusters.d/no_good_config.yml</span><span class="w"></span>
<span class="nn">---</span><span class="w"></span>
<span class="nt">v2</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">job</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">submit_host</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;owens.osc.edu&quot;</span><span class="w">  </span><span class="c1"># This is the head for a login round robin</span><span class="w"></span>
<span class="w">    </span><span class="nt">ssh_hosts</span><span class="p">:</span><span class="w"> </span><span class="c1"># These are the actual login nodes</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">owens-login01.hpc.osc.edu</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">owens-login02.hpc.osc.edu</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="c1"># I need 03 and 04 here!</span><span class="w"></span>
</pre></div>
</div>
<p>In this example Iâve only configured hosts 01 and 02 (above), but I got scheduled on 03 (you can tell
by the âjob nameâ) so the adapter now cannot find my job.</p>
<div class="figure align-default">
<img alt="../../_images/linux_host_undetermined.png" src="../../_images/linux_host_undetermined.png" />
</div>
</div>
<div class="section" id="error-while-loading-shared-libraries">
<h3>error while loading shared libraries<a class="headerlink" href="#error-while-loading-shared-libraries" title="Permalink to this headline">Â¶</a></h3>
<p>The default mounts for singularity are <code class="docutils literal notranslate"><span class="pre">'/etc,/media,/mnt,/opt,/srv,/usr,/var,/users'</span></code>.  Itâs likely
either youâve overwritten this with too few mounts (like /lib, /opt or /usr) or your container lacks
the library in question.</p>
<p>If the library exists on the host, consider mounting it into the container. Otherwise install it in
the container definition and rebuild the container.</p>
</div>
<div class="section" id="the-job-just-exists-with-no-errors">
<h3>The job just exists with no errors.<a class="headerlink" href="#the-job-just-exists-with-no-errors" title="Permalink to this headline">Â¶</a></h3>
<p>This is where turning debug on with <code class="docutils literal notranslate"><span class="pre">debug:</span> <span class="pre">true</span></code> is really going to come in handy.</p>
<p>Enable this, and youâll see the two shell scripts that ran during this job. Open the file ending in
<code class="docutils literal notranslate"><span class="pre">_tmux</span></code> and youâll see something like below.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">SINGULARITY_BINDPATH</span><span class="o">=</span>/usr,/lib,/lib64,/opt
<span class="c1"># ... removed for brevity</span>
<span class="nv">ERROR_PATH</span><span class="o">=</span>/dev/null
<span class="o">({</span>
timeout 28800s /usr/bin/singularity <span class="nb">exec</span>  --pid /users/PZS0714/johrstrom/src/images/shelf/centos.sif /bin/bash --login /users/PZS0714/johrstrom/tmp.73S0QFxC5e_sing
<span class="o">}</span> <span class="p">|</span> tee <span class="s2">&quot;</span><span class="nv">$OUTPUT_PATH</span><span class="s2">&quot;</span><span class="o">)</span> <span class="m">3</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="m">1</span>&gt;<span class="p">&amp;</span><span class="m">2</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">3</span> <span class="p">|</span> tee <span class="s2">&quot;</span><span class="nv">$ERROR_PATH</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Export the SINGULARITY_BINDPATH so youâre sure to have the same mounts, and run this
<code class="docutils literal notranslate"><span class="pre">/usr/bin/singularity</span> <span class="pre">exec</span> <span class="pre">...</span> <span class="pre">tmp.73S0QFxC5e_sing</span></code> command manually on one of the ssh hosts.  This will
emulate what the linuxhost adapter is doing and you should be able to modify and rerun until you fix
the issue.</p>
</div>
<div class="section" id="d-bus-errors">
<h3>D-Bus errors<a class="headerlink" href="#d-bus-errors" title="Permalink to this headline">Â¶</a></h3>
<p>Maybe youâve seen something like below.  Mounting <code class="docutils literal notranslate"><span class="pre">/var</span></code> into the container will likely fix the issue.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Launching desktop <span class="s1">&#39;xfce&#39;</span>...
process <span class="m">195</span>: D-Bus library appears to be incorrectly <span class="nb">set</span> up<span class="p">;</span> failed to <span class="nb">read</span> machine uuid: UUID file <span class="s1">&#39;/etc/machine-id&#39;</span> should contain a hex string of length <span class="m">32</span>, not length <span class="m">0</span>, with no other text
See the manual page <span class="k">for</span> dbus-uuidgen to correct this issue.
  D-Bus not built with -rdynamic so unable to print a backtrace
</pre></div>
</div>
<p>Again, mounting <code class="docutils literal notranslate"><span class="pre">var</span></code> fixed this error too.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Starting system message bus: Could not get password database information <span class="k">for</span> UID of current process: User <span class="s2">&quot;???&quot;</span> unknown or no memory to allocate password entry
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Subsequent versions of the adapter are expected to use <a class="reference external" href="http://man7.org/linux/man-pages/man1/unshare.1.html">unshare</a> for PID namespacing as the default method instead of Singularity. Singularity will continue to be supported.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ccq.html" class="btn btn-neutral float-right" title="Cloudy Cluster" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="sge.html" class="btn btn-neutral float-left" title="Grid Engine" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2024, Ohio Supercomputer Center

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Documentation Versions</span>
    v: 
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      
        <dd><a href="https://OSC.github.io/ood-documentation/latest/installation/resource-manager/linuxhost.html">latest</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-3.0/installation/resource-manager/linuxhost.html">3.0</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-2.0/installation/resource-manager/linuxhost.html">2.0</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.8/installation/resource-manager/linuxhost.html">1.8</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.7/installation/resource-manager/linuxhost.html">1.7</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.6/installation/resource-manager/linuxhost.html">1.6</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.5/installation/resource-manager/linuxhost.html">1.5</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.4/installation/resource-manager/linuxhost.html">1.4</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.3/installation/resource-manager/linuxhost.html">1.3</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.2/installation/resource-manager/linuxhost.html">1.2</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.1/installation/resource-manager/linuxhost.html">1.1</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/release-1.0/installation/resource-manager/linuxhost.html">1.0</a></dd>
      
        <dd><a href="https://OSC.github.io/ood-documentation/develop/installation/resource-manager/linuxhost.html">develop</a></dd>
      
    </dl>
    <dl>
      <dt>On GitHub</dt>
        <dd>
          <a href="http://openondemand.org/">Website</a>
        </dd>
        <dd>
          <a href="https://github.com/OSC/Open-OnDemand">Main Repo</a>
        </dd>
    </dl>
    <hr/>
    Theme provided by <a href="http://www.readthedocs.org">Read the Docs</a>.
  </div>
</div>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>